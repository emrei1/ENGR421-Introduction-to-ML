# -*- coding: utf-8 -*-
"""engr421hw6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FLLvMP3jVgeO1-svIxlixgnxsXF4IVVm
"""

# import packages
import numpy as np
import cvxopt as cvx
import matplotlib.pyplot as plt
import pandas as pd

# import data
data_set_X = np.genfromtxt("hw06_data_set_images.csv", delimiter = ",")
data_set_y = np.genfromtxt("hw06_data_set_labels.csv", delimiter = ",")

split_location = 1000

train_set_X = data_set_X[0:split_location, :]
train_set_y = data_set_y[0:split_location]
test_set_X = data_set_X[split_location : len(data_set_X), :]
test_set_y = data_set_y[split_location : len(data_set_y)]

# process data to get new X vectors
bin_size = 4
number_of_bins = 64

max_pixel_intensity = 255;
cols_train = len(train_set_X)
cols_test = len(test_set_X)

new_train_set_X = np.zeros((cols_train, number_of_bins))
new_test_set_X = np.zeros((cols_test, number_of_bins))

# retrieve new train set
for i in range(len(new_train_set_X)):
  item_i_train = train_set_X[i, :]
  item_i_train = item_i_train.tolist()
  for j in range(number_of_bins):
    pixel_count_train = 0
    for k in range(bin_size):
      pixel_count_train += item_i_train.count(j * bin_size + k)
    new_train_set_X[i, j] = pixel_count_train

# retrieve new test set
for i in range(len(new_test_set_X)):
  item_i_test = test_set_X[i, :]
  item_i_test = item_i_test.tolist()
  for j in range(number_of_bins):
    pixel_count_test = 0
    for k in range(bin_size):
      pixel_count_test += item_i_test.count(j * bin_size + k)
    new_test_set_X[i, j] = pixel_count_test

# adjust matrices according to their ratios of pixel occurances per bin to total pixels
new_train_set_X = new_train_set_X / len(train_set_X[0, :])
new_test_set_X = new_test_set_X / len(test_set_X[0, :])

print(new_train_set_X[0:5, 0:5])
print(new_test_set_X[0:5, 0:5])

# define method to retrieve kernel matrix for two input matrices
def get_min_kernel(X1, X2):
  N = len(train_set_X)
  returned_matrix = np.zeros((N, N))

  for i in range(N):
    for j in range(N):
      returned_matrix[i, j] = np.sum([min(X1[i, b], X2[j, b]) for b in range(number_of_bins)])

  return returned_matrix

# retrieve training and test Kernel matrices
K_train = get_min_kernel(new_train_set_X, new_train_set_X)
K_test = get_min_kernel(new_test_set_X, new_train_set_X)

print(K_train[0:5, 0:5])
print(K_test[0:5, 0:5])

# retrieve model parameters from training kernel for regularization parameter C
def get_alpha_and_w0(C_input):
  returned = []

  yyK = np.matmul(train_set_y[:, None], train_set_y[None, :]) * K_train

  N_train = len(train_set_X)

  C = C_input
  epsilon = 0.001

  # set parameters
  P = cvx.matrix(yyK)
  q = cvx.matrix(-np.ones((N_train, 1)))
  G = cvx.matrix(np.vstack((-np.eye(N_train), np.eye(N_train))))
  h = cvx.matrix(np.vstack((np.zeros((N_train, 1)), C * np.ones((N_train, 1)))))
  A = cvx.matrix(1.0 * train_set_y[None,:])
  b = cvx.matrix(0.0)
                      
  # use cvxopt library to solve QP problems
  result = cvx.solvers.qp(P, q, G, h, A, b)
  alpha = np.reshape(result["x"], N_train)
  alpha[alpha < C * epsilon] = 0
  alpha[alpha > C * (1 - epsilon)] = C

  # find bias parameter
  support_indices, = np.where(alpha != 0)
  active_indices, = np.where(np.logical_and(alpha != 0, alpha < C))
  w0 = np.mean(train_set_y[active_indices] * (1 - np.matmul(yyK[np.ix_(active_indices, support_indices)], alpha[support_indices])))

  returned = [alpha, w0]
  return returned

# calculate predictions on training and test samples for C = 10
result = get_alpha_and_w0(10)
alpha = result[0]
w0 = result[1]

N_train = len(train_set_X)
N_test = len(test_set_X)

f_predicted_train = np.matmul(K_train, train_set_y[:,None] * alpha[:,None]) + w0
f_predicted_test = np.matmul(K_test, train_set_y[:,None] * alpha[:,None]) + w0

# calculate confusion matrix
y_predicted_train = 2 * (f_predicted_train > 0.0) - 1
y_predicted_test = 2 * (f_predicted_test > 0.0) - 1
confusion_matrix_train = pd.crosstab(np.reshape(y_predicted_train, N_train), train_set_y,
                               rownames = ["y_predicted"], colnames = ["y_train"])
confusion_matrix_test = pd.crosstab(np.reshape(y_predicted_test, N_test), test_set_y,
                               rownames = ["y_predicted"], colnames = ["y_train"])

print()
print()
print(confusion_matrix_train)
print(confusion_matrix_test)

# test different models based on different regularization parameters, illustrate results
C_values = [10**-1, 10**-0.5, 10**0, 10**0.5, 10**1, 10**1.5, 10**2, 10**2.5, 10**3]
result_ratios_train = []
result_ratios_test = []

for i in range(len(C_values)):
  result = get_alpha_and_w0(C_values[i])
  alpha = result[0]
  w0 = result[1]

  f_predicted_train = np.matmul(K_train, train_set_y[:,None] * alpha[:,None]) + w0
  f_predicted_test = np.matmul(K_test, train_set_y[:,None] * alpha[:,None]) + w0

  y_predicted_train = 2 * (f_predicted_train > 0.0) - 1
  y_predicted_test = 2 * (f_predicted_test > 0.0) - 1

  count_equal_train = np.sum(np.reshape(y_predicted_train, N_train) == train_set_y)
  count_equal_test = np.sum(np.reshape(y_predicted_test, N_test) == test_set_y)

  ratio_train = count_equal_train / len(y_predicted_train)
  ratio_test = count_equal_test / len(y_predicted_test)

  result_ratios_train.append(ratio_train)
  result_ratios_test.append(ratio_test)

print()
print()
print()

# plot results
plt.figure(figsize = (10, 4))
plt.plot(C_values, result_ratios_train, "b.", markersize = 12, label = 'training')
plt.plot(C_values, result_ratios_test, "r.", markersize = 12, label = 'test')
plt.ylabel('Accuracy')
plt.xlabel('Regularization Parameter (C)')
plt.semilogx(10)
plt.ylim(0.84, 1.02)
plt.legend()
for b in range(len(result_ratios_train) - 1):
  plt.plot([C_values[b], C_values[b + 1]], [result_ratios_train[b], result_ratios_train[b + 1]], color = 'blue')
for b in range(len(result_ratios_test) - 1):
  plt.plot([C_values[b], C_values[b + 1]], [result_ratios_test[b], result_ratios_test[b + 1]], color = 'red')